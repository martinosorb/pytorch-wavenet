{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook gives a quick overview of this WaveNet implementation, i.e. creating the model and the data set, training the model and generating samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset\n",
    "from wavenet_training import *\n",
    "from model_logging import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This is an implementation of WaveNet as it was described in the original paper (https://arxiv.org/abs/1609.03499). Each layer looks like this:\n",
    "\n",
    "```\n",
    "            |----------------------------------------|      *residual*\n",
    "            |                                        |\n",
    "            |    |-- conv -- tanh --|                |\n",
    " -> dilate -|----|                  * ----|-- 1x1 -- + -->  *input*\n",
    "                 |-- conv -- sigm --|     |\n",
    "                                         1x1\n",
    "                                          |\n",
    " ---------------------------------------> + ------------->  *skip*\n",
    "```\n",
    "\n",
    "Each layer dilates the input by a factor of two. After each block the dilation is reset and start from one. You can define the number of layers in each block (``layers``) and the number of blocks (``blocks``). The blocks are followed by two 1x1 convolutions and a softmax output function.\n",
    "Because of the dilation operation, the independent output for multiple successive samples can be calculated efficiently. With ``output_length``, you can define the number these outputs. Empirically, it seems that a large number of skip channels is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu\n"
     ]
    }
   ],
   "source": [
    "# initialize cuda option\n",
    "dtype = torch.FloatTensor # data type\n",
    "ltype = torch.LongTensor # label type\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    ltype = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  WaveNetModel(\n",
      "  (filter_convs): ModuleList(\n",
      "    (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (4): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (5): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (6): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (7): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (8): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (9): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (10): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (11): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (12): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (13): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (14): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (15): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (16): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (17): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (18): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (19): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (20): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (21): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (22): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (23): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (24): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (25): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (26): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (27): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (28): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (29): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (gate_convs): ModuleList(\n",
      "    (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (4): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (5): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (6): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (7): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (8): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (9): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (10): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (11): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (12): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (13): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (14): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (15): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (16): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (17): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (18): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (19): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (20): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (21): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (22): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (23): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (24): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (25): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (26): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (27): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (28): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (29): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (residual_convs): ModuleList(\n",
      "    (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (4): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (6): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (7): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (8): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (9): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (10): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (11): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (12): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (13): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (14): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (15): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (16): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (17): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (18): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (19): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (20): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (21): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (22): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (23): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (24): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (25): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (26): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (27): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (28): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (29): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (skip_convs): ModuleList(\n",
      "    (0): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (1): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (2): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (4): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (5): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (6): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (7): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (8): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (9): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (10): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (11): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (12): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (13): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (14): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (15): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (16): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (17): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (18): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (19): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (20): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (21): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (22): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (23): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (24): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (25): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (26): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (27): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (28): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (29): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (start_conv): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "receptive field:  3070\n",
      "parameter count:  1834592\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetModel(layers=10,\n",
    "                     blocks=3,\n",
    "                     dilation_channels=32,\n",
    "                     residual_channels=32,\n",
    "                     skip_channels=1024,\n",
    "                     end_channels=512, \n",
    "                     output_length=16,\n",
    "                     bias=True)\n",
    "# model = load_latest_model_from('snapshots', use_cuda=use_cuda)\n",
    "\n",
    "print('model: ', model)\n",
    "print('receptive field: ', model.receptive_field)\n",
    "print('parameter count: ', model.parameter_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "To create the data set, you have to specify a path to a data set file. If this file already exists it will be used, if not it will be generated. If you want to generate the data set file (a ``.npz`` file), you have to specify the directory (``file_location``) in which all the audio files you want to use are located. The attribute ``target_length`` specifies the number of successive samples are used as a target and corresponds to the output length of the model. The ``item_length`` defines the number of samples in each item of the dataset and should always be ``model.receptive_field + model.output_length - 1``.\n",
    "\n",
    "```\n",
    "          |----receptive_field----|\n",
    "                                |--output_length--|\n",
    "example:  | | | | | | | | | | | | | | | | | | | | |\n",
    "target:                           | | | | | | | | | |  \n",
    "```\n",
    "To create a test set, you should define a ``test_stride``. Then each ``test_stride``th item will be assigned to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot input\n",
      "the dataset has 598277 items\n"
     ]
    }
   ],
   "source": [
    "data = WavenetDataset(dataset_file='train_samples/bach_chaconne/dataset.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      file_location='train_samples/bach_chaconne',\n",
    "                      test_stride=500)\n",
    "print('the dataset has ' + str(len(data)) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Logging\n",
    "This implementation supports logging with TensorBoard (you need to have TensorFlow installed). You can even generate audio samples from the current snapshot of the model during training. This will happen in a background thread on the cpu, so it will not interfere with the actual training but will be rather slow. If you don't have TensorFlow, you can use the standard logger that will print out to the console.\n",
    "The trainer uses Adam as default optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4135/2632646405.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m logger = TensorboardLogger(log_interval=200,\n\u001b[0m\u001b[1;32m     20\u001b[0m                            \u001b[0mvalidation_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                            \u001b[0mgenerate_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/emg/wavenet/model_logging.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_interval, validation_interval, generate_interval, trainer, generate_function, log_dir)\u001b[0m\n\u001b[1;32m     67\u001b[0m                  \u001b[0mgenerate_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                  log_dir='logs'):\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "def generate_and_log_samples(step):\n",
    "    sample_length=32000\n",
    "    gen_model = load_latest_model_from('snapshots', use_cuda=False)\n",
    "    print(\"start generating...\")\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[0.5])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    logger.audio_summary('temperature_0.5', tf_samples, step, sr=16000)\n",
    "\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[1.])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    logger.audio_summary('temperature_1.0', tf_samples, step, sr=16000)\n",
    "    print(\"audio clips generated\")\n",
    "\n",
    "\n",
    "logger = TensorboardLogger(log_interval=200,\n",
    "                           validation_interval=400,\n",
    "                           generate_interval=1000,\n",
    "                           generate_function=generate_and_log_samples,\n",
    "                           log_dir=\"logs/chaconne_model\")\n",
    "\n",
    "# logger = Logger(log_interval=200,\n",
    "#                 validation_interval=400,\n",
    "#                 generate_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martino/.pyenv/versions/3.9.6/lib/python3.9/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/martino/.pyenv/versions/3.9.6/lib/python3.9/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 50: 5.222252149581909\n",
      "one training step takes approximately 0.43947763442993165 seconds)\n",
      "loss at step 100: 5.117213668823243\n",
      "loss at step 150: 4.79701639175415\n",
      "loss at step 200: 4.193910746574402\n",
      "validation loss: 4.2310436344146725\n",
      "validation accuracy: 3.458889816360601%\n",
      "loss at step 250: 3.9112370204925537\n",
      "loss at step 300: 3.785445818901062\n",
      "loss at step 350: 3.7021886920928955\n",
      "loss at step 400: 3.6253786849975587\n",
      "validation loss: 3.865913257598877\n",
      "validation accuracy: 7.538606010016695%\n",
      "loss at step 450: 3.5864906406402586\n",
      "loss at step 500: 3.5864750480651857\n",
      "loss at step 550: 3.4925302171707155\n",
      "loss at step 600: 3.513213257789612\n",
      "validation loss: 3.740694621404012\n",
      "validation accuracy: 7.924666110183639%\n",
      "loss at step 650: 3.4819477891921995\n",
      "loss at step 700: 3.434294285774231\n",
      "loss at step 750: 3.467810230255127\n",
      "loss at step 800: 3.4292041635513306\n",
      "validation loss: 3.6667398516337077\n",
      "validation accuracy: 8.04465776293823%\n",
      "loss at step 850: 3.4123240756988524\n",
      "loss at step 900: 3.4263628244400026\n",
      "loss at step 950: 3.3890484523773194\n",
      "loss at step 1000: 3.396002860069275\n",
      "validation loss: 3.623418083190918\n",
      "validation accuracy: 8.571577629382304%\n",
      "loss at step 1050: 3.408003888130188\n",
      "loss at step 1100: 3.326028208732605\n",
      "loss at step 1150: 3.334118242263794\n",
      "loss at step 1200: 3.310981049537659\n",
      "validation loss: 3.570417003631592\n",
      "validation accuracy: 8.35246243739566%\n",
      "loss at step 1250: 3.334334692955017\n",
      "loss at step 1300: 3.3383894968032837\n",
      "loss at step 1350: 3.312505297660828\n",
      "loss at step 1400: 3.3157224082946777\n",
      "validation loss: 3.4953354740142824\n",
      "validation accuracy: 8.936769616026712%\n",
      "loss at step 1450: 3.3051903009414674\n",
      "loss at step 1500: 3.2786059617996215\n",
      "loss at step 1550: 3.247499985694885\n",
      "loss at step 1600: 3.248153877258301\n",
      "validation loss: 3.45786847114563\n",
      "validation accuracy: 8.978505843071787%\n",
      "loss at step 1650: 3.3028104209899904\n",
      "loss at step 1700: 3.282320246696472\n",
      "loss at step 1750: 3.2941441106796265\n",
      "loss at step 1800: 3.2626842784881593\n",
      "validation loss: 3.433236707051595\n",
      "validation accuracy: 9.129799666110184%\n",
      "loss at step 1850: 3.274795994758606\n",
      "loss at step 1900: 3.224496374130249\n",
      "loss at step 1950: 3.2446209621429443\n",
      "loss at step 2000: 3.2266933155059814\n",
      "validation loss: 3.4091409174601237\n",
      "validation accuracy: 8.99415692821369%\n",
      "loss at step 2050: 3.2495261335372927\n",
      "loss at step 2100: 3.2214161062240603\n",
      "loss at step 2150: 3.2266827392578126\n",
      "loss at step 2200: 3.240542006492615\n",
      "validation loss: 3.4649224376678465\n",
      "validation accuracy: 6.536936560934892%\n",
      "loss at step 2250: 3.2057385063171386\n",
      "loss at step 2300: 3.2199279928207396\n",
      "loss at step 2350: 3.1904787588119508\n",
      "loss at step 2400: 3.202109251022339\n",
      "validation loss: 3.3698509693145753\n",
      "validation accuracy: 9.5419449081803%\n",
      "loss at step 2450: 3.1985316276550293\n",
      "loss at step 2500: 3.1997658967971803\n",
      "loss at step 2550: 3.2049197435379027\n",
      "loss at step 2600: 3.191232514381409\n",
      "validation loss: 3.3642963854471843\n",
      "validation accuracy: 9.468906510851419%\n",
      "loss at step 2650: 3.188254909515381\n",
      "loss at step 2700: 3.170590023994446\n",
      "loss at step 2750: 3.1462454748153688\n",
      "loss at step 2800: 3.1794317722320558\n",
      "validation loss: 3.3434969425201415\n",
      "validation accuracy: 9.708889816360601%\n",
      "loss at step 2850: 3.176769561767578\n"
     ]
    }
   ],
   "source": [
    "trainer = WavenetTrainer(model=model.cuda(),\n",
    "                         dataset=data,\n",
    "                         lr=0.001,\n",
    "                         snapshot_path='snapshots',\n",
    "                         snapshot_name='chaconne_model',\n",
    "                         snapshot_interval=1000,\n",
    "                         device='cuda',\n",
    "#                          logger=logger,\n",
    "#                          dtype=dtype,\n",
    "#                          ltype=ltype\n",
    ")\n",
    "\n",
    "print('start training...')\n",
    "trainer.train(batch_size=16,\n",
    "              epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating\n",
    "This model has the Fast Wavenet Generation Algorithm (https://arxiv.org/abs/1611.09482) implemented. This might run faster on the cpu. You can give some starting data (of at least the length of receptive field) or let the model generate from zero. In my experience, a temperature between 0.5 and 1.0 yields the best results, but this may depend on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martino/.pyenv/versions/3.9.6/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'wavenet_model.WaveNetModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/martino/.pyenv/versions/3.9.6/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/martino/.pyenv/versions/3.9.6/lib/python3.9/site-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "# can also load directly from here\n",
    "model = torch.load(\"./snapshots/chaconne_model_2017-12-28_16-44-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[32, 1]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25584/2244183698.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m generated = model.generate_fast(num_samples=160000,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                  \u001b[0mfirst_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                  \u001b[0mprogress_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprog_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/emg/wavenet/wavenet_model.py\u001b[0m in \u001b[0;36mgenerate_fast\u001b[0;34m(self, num_samples, first_samples, temperature, regularize, progress_callback, progress_interval)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# fill queues with given samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_given_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             x = self.wavenet(input,\n\u001b[0m\u001b[1;32m    274\u001b[0m                              dilation_func=self.queue_dilate)\n\u001b[1;32m    275\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/emg/wavenet/wavenet_model.py\u001b[0m in \u001b[0;36mwavenet\u001b[0;34m(self, input, dilation_func)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_dilation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdilation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_dilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;31m# dilated convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/emg/wavenet/wavenet_model.py\u001b[0m in \u001b[0;36mqueue_dilate\u001b[0;34m(self, input, dilation, init_dilation, i)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_dilate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_dilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mqueue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilated_queues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menqueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         x = queue.dequeue(num_deq=self.kernel_size,\n\u001b[1;32m    192\u001b[0m                           dilation=dilation)\n",
      "\u001b[0;32m~/Work/emg/wavenet/wavenet_modules.py\u001b[0m in \u001b[0;36menqueue\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menqueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[32, 1]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "start_data = data[250000][0] # use start data from the data set\n",
    "start_data = torch.max(start_data, 0)[1] # convert one hot vectors to integers\n",
    "\n",
    "def prog_callback(step, total_steps):\n",
    "    print(str(100 * step // total_steps) + \"% generated\")\n",
    "\n",
    "model.device = 'cpu'\n",
    "generated = model.generate_fast(num_samples=160000,\n",
    "                                 first_samples=start_data,\n",
    "                                 progress_callback=prog_callback,\n",
    "                                 progress_interval=1000,\n",
    "                                 temperature=1.0,\n",
    "                                 regularize=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(generated, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
